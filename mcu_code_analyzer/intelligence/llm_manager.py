"""
LLMç®¡ç†å™¨ - æ”¯æŒå¤šç§LLMæœåŠ¡çš„ç»Ÿä¸€æ¥å£
"""

import json
import requests
import asyncio
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable
import subprocess
import time
import hashlib
import hmac
import base64
from datetime import datetime
from dataclasses import dataclass
from utils.logger import logger, log_decorator, performance_monitor
from utils.config import config, LLMConfig


@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®ç±»"""
    content: str
    success: bool
    error_message: str = ""
    usage: Dict[str, int] = None
    model: str = ""
    provider: str = ""
    response_time: float = 0.0


class LLMClient(ABC):
    """LLMå®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.session = requests.Session()
        self._setup_session()
    
    def _setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        if self.config.api_key:
            self.session.headers.update({
                "Authorization": f"Bearer {self.config.api_key}",
                "Content-Type": "application/json"
            })
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    @abstractmethod
    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        pass


class OpenAIClient(LLMClient):
    """OpenAIå…¼å®¹APIå®¢æˆ·ç«¯"""
    
    @log_decorator
    @performance_monitor
    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """åŒæ­¥ç”Ÿæˆ"""
        start_time = time.time()
        
        try:
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "model": self.config.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": kwargs.get("temperature", self.config.temperature),
                "max_tokens": kwargs.get("max_tokens", self.config.max_tokens)
            }
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                f"{self.config.base_url}/chat/completions",
                json=request_data,
                timeout=kwargs.get("timeout", self.config.timeout)
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                usage = result.get("usage", {})
                
                logger.info(f"OpenAI APIè°ƒç”¨æˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                return LLMResponse(
                    content=content,
                    success=True,
                    usage=usage,
                    model=self.config.model,
                    provider="openai",
                    response_time=response_time
                )
            else:
                error_msg = f"APIé”™è¯¯: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return LLMResponse(
                    content="",
                    success=False,
                    error_message=error_msg,
                    provider="openai",
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}"
            logger.error(error_msg)
            return LLMResponse(
                content="",
                success=False,
                error_message=error_msg,
                provider="openai",
                response_time=response_time
            )
    
    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, prompt, **kwargs)
    
    def is_available(self) -> bool:
        """æ£€æŸ¥OpenAI APIæ˜¯å¦å¯ç”¨"""
        try:
            response = self.session.get(
                f"{self.config.base_url}/models",
                timeout=5
            )
            return response.status_code == 200
        except:
            return False


class OllamaClient(LLMClient):
    """Ollamaæœ¬åœ°LLMå®¢æˆ·ç«¯"""
    
    @log_decorator
    @performance_monitor
    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """åŒæ­¥ç”Ÿæˆ"""
        start_time = time.time()
        
        try:
            request_data = {
                "model": self.config.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", self.config.temperature),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_predict": kwargs.get("max_tokens", self.config.max_tokens)
                }
            }
            
            response = self.session.post(
                f"{self.config.base_url}/api/generate",
                json=request_data,
                timeout=kwargs.get("timeout", self.config.timeout)
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")
                
                logger.info(f"Ollamaè°ƒç”¨æˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                return LLMResponse(
                    content=content,
                    success=True,
                    model=self.config.model,
                    provider="ollama",
                    response_time=response_time
                )
            else:
                error_msg = f"Ollamaé”™è¯¯: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return LLMResponse(
                    content="",
                    success=False,
                    error_message=error_msg,
                    provider="ollama",
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"Ollamaè°ƒç”¨å¤±è´¥: {e}"
            logger.error(error_msg)
            return LLMResponse(
                content="",
                success=False,
                error_message=error_msg,
                provider="ollama",
                response_time=response_time
            )
    
    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, prompt, **kwargs)
    
    def is_available(self) -> bool:
        """æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨"""
        try:
            logger.info(f"Testing Ollama connection to: {self.config.base_url}/api/tags")
            response = self.session.get(f"{self.config.base_url}/api/tags", timeout=5)
            logger.info(f"Ollama response status: {response.status_code}")
            if response.status_code == 200:
                logger.info("âœ… Ollama connection successful")
                return True
            else:
                logger.warning(f"âŒ Ollama connection failed with status: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ Ollama connection error: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.config.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
        except Exception as e:
            logger.error(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []


class LocalCommandClient(LLMClient):
    """æœ¬åœ°å‘½ä»¤è¡ŒLLMå®¢æˆ·ç«¯"""
    
    def __init__(self, config: LLMConfig, command_template: str = None):
        super().__init__(config)
        self.command_template = command_template or "ollama run {model} '{prompt}'"
    
    @log_decorator
    @performance_monitor
    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """é€šè¿‡å‘½ä»¤è¡Œè°ƒç”¨æœ¬åœ°LLM"""
        start_time = time.time()
        
        try:
            # æ¸…ç†æç¤ºè¯ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            clean_prompt = prompt.replace("'", "\\'").replace('"', '\\"')
            
            command = self.command_template.format(
                model=self.config.model,
                prompt=clean_prompt
            )
            
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=kwargs.get("timeout", self.config.timeout)
            )
            
            response_time = time.time() - start_time
            
            if result.returncode == 0:
                content = result.stdout.strip()
                logger.info(f"æœ¬åœ°å‘½ä»¤è°ƒç”¨æˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                return LLMResponse(
                    content=content,
                    success=True,
                    model=self.config.model,
                    provider="local_command",
                    response_time=response_time
                )
            else:
                error_msg = f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}"
                logger.error(error_msg)
                return LLMResponse(
                    content="",
                    success=False,
                    error_message=error_msg,
                    provider="local_command",
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"æœ¬åœ°å‘½ä»¤è°ƒç”¨å¤±è´¥: {e}"
            logger.error(error_msg)
            return LLMResponse(
                content="",
                success=False,
                error_message=error_msg,
                provider="local_command",
                response_time=response_time
            )
    
    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, prompt, **kwargs)
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°å‘½ä»¤æ˜¯å¦å¯ç”¨"""
        try:
            # å°è¯•æ‰§è¡Œä¸€ä¸ªç®€å•çš„å‘½ä»¤æ¥æ£€æŸ¥å¯ç”¨æ€§
            test_command = self.command_template.format(
                model=self.config.model,
                prompt="test"
            )
            result = subprocess.run(
                test_command,
                shell=True,
                capture_output=True,
                timeout=10
            )
            return result.returncode == 0
        except:
            return False


class TencentClient(LLMClient):
    """è…¾è®¯äº‘LLMå®¢æˆ·ç«¯"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.secret_id = config.api_key.split(':')[0] if ':' in config.api_key else config.api_key
        self.secret_key = config.api_key.split(':')[1] if ':' in config.api_key else ""
        self.region = getattr(config, 'region', 'ap-beijing')

    def _generate_signature(self, method: str, uri: str, query: str, headers: dict, payload: str) -> str:
        """ç”Ÿæˆè…¾è®¯äº‘APIç­¾å"""
        # æ„å»ºè§„èŒƒè¯·æ±‚ä¸²
        canonical_headers = ""
        signed_headers = ""

        # å¯¹headersæŒ‰keyæ’åº
        sorted_headers = sorted(headers.items())
        for key, value in sorted_headers:
            canonical_headers += f"{key.lower()}:{value}\n"
            signed_headers += f"{key.lower()};"

        signed_headers = signed_headers[:-1]  # ç§»é™¤æœ€åçš„åˆ†å·

        canonical_request = f"{method}\n{uri}\n{query}\n{canonical_headers}\n{signed_headers}\n{hashlib.sha256(payload.encode('utf-8')).hexdigest()}"

        # æ„å»ºå¾…ç­¾åå­—ç¬¦ä¸²
        algorithm = "TC3-HMAC-SHA256"
        timestamp = str(int(time.time()))
        date = datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d')
        credential_scope = f"{date}/hunyuan/tc3_request"
        string_to_sign = f"{algorithm}\n{timestamp}\n{credential_scope}\n{hashlib.sha256(canonical_request.encode('utf-8')).hexdigest()}"

        # è®¡ç®—ç­¾å
        secret_date = hmac.new(f"TC3{self.secret_key}".encode('utf-8'), date.encode('utf-8'), hashlib.sha256).digest()
        secret_service = hmac.new(secret_date, "hunyuan".encode('utf-8'), hashlib.sha256).digest()
        secret_signing = hmac.new(secret_service, "tc3_request".encode('utf-8'), hashlib.sha256).digest()
        signature = hmac.new(secret_signing, string_to_sign.encode('utf-8'), hashlib.sha256).hexdigest()

        return signature, timestamp, signed_headers

    @log_decorator
    @performance_monitor
    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """åŒæ­¥ç”Ÿæˆ"""
        start_time = time.time()

        try:
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "Model": self.config.model or "hunyuan-lite",
                "Messages": [{"Role": "user", "Content": prompt}],
                "Temperature": kwargs.get("temperature", self.config.temperature),
                "TopP": kwargs.get("top_p", 0.8),
                "Stream": False
            }

            payload = json.dumps(request_data)

            # æ„å»ºè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json; charset=utf-8",
                "Host": "hunyuan.tencentcloudapi.com",
                "X-TC-Action": "ChatCompletions",
                "X-TC-Version": "2023-09-01",
                "X-TC-Region": self.region
            }

            # ç”Ÿæˆç­¾å
            signature, timestamp, signed_headers = self._generate_signature(
                "POST", "/", "", headers, payload
            )

            # æ·»åŠ æˆæƒå¤´
            authorization = f"TC3-HMAC-SHA256 Credential={self.secret_id}/{datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d')}/hunyuan/tc3_request, SignedHeaders={signed_headers}, Signature={signature}"
            headers["Authorization"] = authorization
            headers["X-TC-Timestamp"] = timestamp

            # å‘é€è¯·æ±‚
            response = self.session.post(
                "https://hunyuan.tencentcloudapi.com/",
                headers=headers,
                data=payload,
                timeout=kwargs.get("timeout", self.config.timeout)
            )

            response_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()

                if "Error" in result["Response"]:
                    error_msg = f"è…¾è®¯APIé”™è¯¯: {result['Response']['Error']['Message']}"
                    logger.error(error_msg)
                    return LLMResponse(
                        content="",
                        success=False,
                        error_message=error_msg,
                        provider="tencent",
                        response_time=response_time
                    )

                choices = result["Response"].get("Choices", [])
                if choices:
                    content = choices[0]["Message"]["Content"]
                    usage = result["Response"].get("Usage", {})

                    logger.info(f"è…¾è®¯APIè°ƒç”¨æˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                    return LLMResponse(
                        content=content,
                        success=True,
                        usage=usage,
                        model=self.config.model,
                        provider="tencent",
                        response_time=response_time
                    )
                else:
                    error_msg = "è…¾è®¯APIè¿”å›ç©ºå“åº”"
                    logger.error(error_msg)
                    return LLMResponse(
                        content="",
                        success=False,
                        error_message=error_msg,
                        provider="tencent",
                        response_time=response_time
                    )
            else:
                error_msg = f"è…¾è®¯APIé”™è¯¯: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return LLMResponse(
                    content="",
                    success=False,
                    error_message=error_msg,
                    provider="tencent",
                    response_time=response_time
                )

        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"è…¾è®¯APIè°ƒç”¨å¤±è´¥: {e}"
            logger.error(error_msg)
            return LLMResponse(
                content="",
                success=False,
                error_message=error_msg,
                provider="tencent",
                response_time=response_time
            )

    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, prompt, **kwargs)

    def is_available(self) -> bool:
        """æ£€æŸ¥è…¾è®¯APIæ˜¯å¦å¯ç”¨"""
        if not self.secret_id or not self.secret_key:
            return False

        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            test_response = self.generate("test", timeout=10)
            return test_response.success or "è®¤è¯" not in test_response.error_message.lower()
        except:
            return False


class LLMManager:
    """LLMç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªLLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.clients: Dict[str, LLMClient] = {}
        self.current_provider = None
        self.fallback_order = ['ollama', 'openai', 'tencent', 'custom']
        self._initialize_clients()
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        logger.info("åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        
        # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯
        ollama_config = config.get_llm_config('ollama')
        if ollama_config:
            ollama_client = OllamaClient(ollama_config)
            self.clients['ollama'] = ollama_client
            if ollama_client.is_available():
                logger.info("âœ… Ollamaå®¢æˆ·ç«¯å¯ç”¨")
                if not self.current_provider:
                    self.current_provider = 'ollama'
            else:
                logger.warning("âš ï¸ Ollamaå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        openai_config = config.get_llm_config('openai')
        if openai_config and openai_config.api_key:
            openai_client = OpenAIClient(openai_config)
            self.clients['openai'] = openai_client
            if openai_client.is_available():
                logger.info("âœ… OpenAIå®¢æˆ·ç«¯å¯ç”¨")
                if not self.current_provider:
                    self.current_provider = 'openai'
            else:
                logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
        # åˆå§‹åŒ–è…¾è®¯å®¢æˆ·ç«¯
        tencent_config = config.get_llm_config('tencent')
        if tencent_config and tencent_config.api_key:
            tencent_client = TencentClient(tencent_config)
            self.clients['tencent'] = tencent_client
            if tencent_client.is_available():
                logger.info("âœ… è…¾è®¯äº‘APIå®¢æˆ·ç«¯å¯ç”¨")
                if not self.current_provider:
                    self.current_provider = 'tencent'
            else:
                logger.warning("âš ï¸ è…¾è®¯äº‘APIå®¢æˆ·ç«¯ä¸å¯ç”¨")

        # åˆå§‹åŒ–è‡ªå®šä¹‰å®¢æˆ·ç«¯
        custom_config = config.get_llm_config('custom')
        if custom_config and custom_config.base_url:
            custom_client = OpenAIClient(custom_config)  # ä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
            self.clients['custom'] = custom_client
            if custom_client.is_available():
                logger.info("âœ… è‡ªå®šä¹‰APIå®¢æˆ·ç«¯å¯ç”¨")
                if not self.current_provider:
                    self.current_provider = 'custom'
            else:
                logger.warning("âš ï¸ è‡ªå®šä¹‰APIå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
        if self.current_provider:
            logger.info(f"ğŸ¯ å½“å‰ä½¿ç”¨: {self.current_provider}")
        else:
            logger.warning("âŒ æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
    
    @log_decorator
    def generate(self, prompt: str, provider: str = None, **kwargs) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬ - æ”¯æŒè‡ªåŠ¨é‡è¯•å’Œé™çº§"""
        if provider and provider in self.clients:
            # ä½¿ç”¨æŒ‡å®šçš„æä¾›å•†
            return self.clients[provider].generate(prompt, **kwargs)
        
        # ä½¿ç”¨å½“å‰æä¾›å•†æˆ–æŒ‰ä¼˜å…ˆçº§å°è¯•
        providers_to_try = [self.current_provider] if self.current_provider else []
        providers_to_try.extend([p for p in self.fallback_order if p not in providers_to_try])
        
        for provider_name in providers_to_try:
            if provider_name in self.clients:
                logger.info(f"å°è¯•ä½¿ç”¨ {provider_name}")
                response = self.clients[provider_name].generate(prompt, **kwargs)
                
                if response.success:
                    self.current_provider = provider_name
                    return response
                else:
                    logger.warning(f"{provider_name} è°ƒç”¨å¤±è´¥: {response.error_message}")
        
        # All clients failed
        logger.error("All LLM clients failed")
        return LLMResponse(
            content="",
            success=False,
            error_message="All LLM clients unavailable",
            provider="none"
        )

    async def generate_async(self, prompt: str, provider: str = None, **kwargs) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""
        if provider and provider in self.clients:
            return await self.clients[provider].generate_async(prompt, **kwargs)

        providers_to_try = [self.current_provider] if self.current_provider else []
        providers_to_try.extend([p for p in self.fallback_order if p not in providers_to_try])

        for provider_name in providers_to_try:
            if provider_name in self.clients:
                response = await self.clients[provider_name].generate_async(prompt, **kwargs)
                if response.success:
                    self.current_provider = provider_name
                    return response

        return LLMResponse(
            content="",
            success=False,
            error_message="All LLM clients unavailable",
            provider="none"
        )

    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„LLMæœåŠ¡"""
        return len(self.get_available_providers()) > 0

    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        available = []
        for provider, client in self.clients.items():
            if client.is_available():
                available.append(provider)
        return available

    def get_provider_info(self) -> Dict[str, Dict[str, any]]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        info = {}
        for provider, client in self.clients.items():
            info[provider] = {
                'available': client.is_available(),
                'model': client.config.model,
                'base_url': client.config.base_url,
                'provider_type': type(client).__name__
            }
        return info

    def set_current_provider(self, provider: str) -> bool:
        """è®¾ç½®å½“å‰æä¾›å•†"""
        if provider in self.clients and self.clients[provider].is_available():
            self.current_provider = provider
            logger.info(f"åˆ‡æ¢åˆ°æä¾›å•†: {provider}")
            return True
        else:
            logger.error(f"æä¾›å•†ä¸å¯ç”¨: {provider}")
            return False

    def add_client(self, name: str, client: LLMClient):
        """æ·»åŠ æ–°çš„LLMå®¢æˆ·ç«¯"""
        self.clients[name] = client
        if client.is_available() and not self.current_provider:
            self.current_provider = name
        logger.info(f"æ·»åŠ LLMå®¢æˆ·ç«¯: {name}")

    def remove_client(self, name: str):
        """ç§»é™¤LLMå®¢æˆ·ç«¯"""
        if name in self.clients:
            del self.clients[name]
            if self.current_provider == name:
                # é‡æ–°é€‰æ‹©å¯ç”¨çš„æä¾›å•†
                available = self.get_available_providers()
                self.current_provider = available[0] if available else None
            logger.info(f"ç§»é™¤LLMå®¢æˆ·ç«¯: {name}")

    def test_connection(self, provider: str = None) -> Dict[str, any]:
        """æµ‹è¯•è¿æ¥"""
        if provider:
            providers_to_test = [provider] if provider in self.clients else []
        else:
            providers_to_test = list(self.clients.keys())

        results = {}
        for provider_name in providers_to_test:
            client = self.clients[provider_name]
            start_time = time.time()

            try:
                available = client.is_available()
                response_time = time.time() - start_time

                results[provider_name] = {
                    'available': available,
                    'response_time': response_time,
                    'model': client.config.model,
                    'error': None
                }

                if available:
                    logger.info(f"âœ… {provider_name} è¿æ¥æ­£å¸¸ ({response_time:.2f}s)")
                else:
                    logger.warning(f"âŒ {provider_name} è¿æ¥å¤±è´¥")

            except Exception as e:
                response_time = time.time() - start_time
                results[provider_name] = {
                    'available': False,
                    'response_time': response_time,
                    'model': client.config.model,
                    'error': str(e)
                }
                logger.error(f"âŒ {provider_name} æµ‹è¯•å¤±è´¥: {e}")

        return results

    def test_connection_with_config(self, provider: str, test_config: LLMConfig) -> Dict[str, any]:
        """ä½¿ç”¨æŒ‡å®šé…ç½®æµ‹è¯•è¿æ¥"""
        import time

        start_time = time.time()

        try:
            # åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
            if provider == 'ollama':
                test_client = OllamaClient(test_config)
            elif provider == 'tencent_api':
                test_client = TencentClient(test_config)
            elif provider in ['openai', 'custom']:
                test_client = OpenAIClient(test_config)
            else:
                return {
                    'success': False,
                    'error': f'Unsupported provider: {provider}',
                    'response_time': 0
                }

            # æµ‹è¯•è¿æ¥
            available = test_client.is_available()
            response_time = int((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

            if available:
                # å°è¯•å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
                try:
                    test_response = test_client.generate("Hello", max_tokens=5, timeout=10)
                    if test_response and test_response.success:
                        return {
                            'success': True,
                            'response_time': response_time,
                            'provider': provider,
                            'model': test_config.model,
                            'response_content': test_response.content[:50] + "..." if len(test_response.content) > 50 else test_response.content
                        }
                    else:
                        error_detail = test_response.error_message if test_response else "No response object"
                        return {
                            'success': False,
                            'error': f'Test request failed: {error_detail}',
                            'response_time': response_time
                        }
                except Exception as e:
                    return {
                        'success': False,
                        'error': f'Test request exception: {str(e)}',
                        'response_time': response_time
                    }
            else:
                return {
                    'success': False,
                    'error': 'Service not available',
                    'response_time': response_time
                }

        except Exception as e:
            response_time = int((time.time() - start_time) * 1000)
            return {
                'success': False,
                'error': str(e),
                'response_time': response_time
            }

    def get_models(self, provider: str = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = {}

        providers_to_check = [provider] if provider else self.clients.keys()

        for provider_name in providers_to_check:
            if provider_name in self.clients:
                client = self.clients[provider_name]
                if hasattr(client, 'get_available_models'):
                    try:
                        models[provider_name] = client.get_available_models()
                    except Exception as e:
                        logger.error(f"è·å–{provider_name}æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
                        models[provider_name] = []
                else:
                    models[provider_name] = [client.config.model]

        return models

    def update_config(self, provider: str, new_config: LLMConfig):
        """æ›´æ–°æä¾›å•†é…ç½®"""
        if provider in self.clients:
            # åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹
            if provider == 'ollama':
                new_client = OllamaClient(new_config)
            elif provider in ['openai', 'custom']:
                new_client = OpenAIClient(new_config)
            elif provider == 'tencent':
                new_client = TencentClient(new_config)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: {provider}")
                return False

            # æ›¿æ¢å®¢æˆ·ç«¯
            self.clients[provider] = new_client
            config.update_llm_config(provider, new_config)

            logger.info(f"æ›´æ–°{provider}é…ç½®æˆåŠŸ")
            return True
        else:
            logger.error(f"æä¾›å•†ä¸å­˜åœ¨: {provider}")
            return False


# å…¨å±€LLMç®¡ç†å™¨å®ä¾‹
llm_manager = LLMManager()
